---
title: "Probioitics Skeleton"
author: "Suzie Hoops & Carter Linhardt"
date: "10/01/2018"
output: html_document
---

```{r setup, include = FALSE, message = FALSE}
# GENERAL SET-UP

# Setting up the document
knitr::opts_chunk$set(echo = TRUE)

# Load necessary packages (if you need to install use install.packages())
require(knitr)
require(vegan)
require(tidyverse)
require(phyloseq)
#note: had to install phyloseq from source using source("https://raw.githubusercontent.com/joey711/phyloseq/master/inst/scripts/installer.R")
require(ggplot2)

# Note: if you change to PDF or Word document, may want to specify resolution/format (example below)
# set root directory to Dropbox folder
opts_knit$set(root.dir = gsub("/bin", "", getwd()))
# formatting chunks - not recommended unless creating pngs and pdfs (we will add this later)
#opts_chunk$set(echo = TRUE, fig.path = "Figs/", dev = c("png", "pdf"), dpi = 300)
```

## Loading & Cleaning Data

```{r load data, include = FALSE, message = FALSE}
# LOAD DATA
getwd()
# Load raw taxa file (from BURST)
tax <- read.delim("data/r/syafinaz_gg97_taxatable.txt", row = 1, stringsAsFactors = FALSE)

# Load raw otu file
# CARTER : write something to bring in otufile
otu <- read.table("data/r/syafinaz_gg97_otutable.txt", sep='\t', head=T, row=1, comment='', quote="")
KEGG <- NULL # store last column of otu file if descriptions
if (colnames(otu)[ncol(otu)] == "KEGG_Pathways") {
  KEGG <- setNames(otu[,ncol(otu)], rownames(otu)) #set names of rows to descriptions
  otu <- otu[,-ncol(otu)] # remove last column
}

# Load mapfile for metadata
map <- read.table("data/r/sya_collab_mastermap.txt", sep = "\t", head = TRUE, row = 1, comment = "")

# Load tree file from GreenGenes
tree <- read_tree_greengenes("data/r/gg97.tre")
```

### Map File

The map file contains fewer than the expected 160 samples (4 from each of the 40 patients). So, we will need to restrict our analysis to only those samples included in the data. Further down the road we will have to keep in mind some subjects may have incomplete data.

```{r mapfile}
# MAP FILE CLEAN-UP

########## Mapfile clean-up/check ##########
subjects <- unique(map$SubjectID)  #unique subject id's
nsub <- length(subjects)           #number of subjects (40)
samples <- unique(rownames(map))   #unique sample id's (stored in rownames)
nsamp <- length(samples)           #number of samples (157)
```

### Taxa/OTU tables

We have already run the raw data through shi7 and BURST, so now we want to clean up these OTU tables for alpha and beta diversity analyses. Here we get to cleaning the data in preparation for diversity analyses. The loading data was already done, here we need to fill in code for tampering with the data.

```{r clean data}
# FILTER TAXA & OTU DATA

########## Taxa Clean-up ##########

# restrict to only the samples in our mapfile
tax <- tax[,colnames(tax) %in% samples]

# drop last column with taxonomy info.
tax_info <- tax$taxonomy
tax <- tax[,colnames(tax) != "taxonomy"]  
#otu files have extra col "KEGG_Pathways" that should be removed as well, use the names as row names if they aren't already present

# check column sums for total sample reads
check <- as.data.frame(sort(colSums(tax)))
check #view check output looks like only two samples have fewer than 500

# drop samples with low read counts
tax <- tax[,colSums(tax) > 500] #500 seems like a good threshold for the taxatable

# drop taxa with zero counts
tax <- tax[rowSums(tax) > 0,]

# look for non-bacterial hits, store them separately
tax_non_bact <- tax[-grep("k__Bacteria", rownames(tax)),]

# get taxa at species level, appending NA's for unknowns (tax_s is taxatable at species level)
tax_s <- rowsum(tax,rownames(tax))
rownames(tax_s) = sapply(strsplit(rownames(tax_s),";"),function(x) paste(x[1:7],collapse=";"))

# normalize for relative abundance of taxa per sample (colSums should be 100% - 1.0)
tax_norm <- sweep(tax_s, 2, colSums(tax_s), "/") #sweep by columns(2), dividing to get relative abund.("/")
# check that colsums are 1 after this

# drop low abundance
## this removes 475 taxa, leaving 117 taxa of relatively high abundance
tax_norm_dla <- tax_norm[rowMeans(tax_norm) >= 0.0001,]  #removes taxa with an avg. abundance less than .01%
tax_s <- tax_s[rownames(tax_s) %in% rownames(tax_norm_dla),]   #reduce tax_s file to just these species

# normalize again after removal
tax_norm_s <- sweep(tax_s, 2, colSums(tax_s), "/")

# store all sample names for later
sample_names <- colnames(tax)

# create clr version - make sure it is converted to integer
tax_clr_s <- tax_s*1.0

# transpose for imputation - taxa as columns
tax_clr_s <- t(tax_clr_s)

# CLR transformation (imputation)
eps <- 0.5 # epsilon value
tax_clr_s <- tax_clr_s * (1 - rowSums(tax_clr_s==0) * eps / rowSums(tax_clr_s)) #shift values
tax_clr_s[tax_clr_s == 0] <- eps #assign epsilon value to all zero entries
ls <- log(tax_clr_s)  #take a logarithm of shifted values
tax_clr_s <- ls - rowMeans(ls) #difference from mean logarithm
tax_clr_s <- tax_clr_s[, !is.nan(colSums(tax_clr_s))]
tax_s <- as.data.frame(tax_clr_s)


########## OTU Clean-up ##########

# CARTER : Complete the above steps for the otu table. Look at the otu table to make sure you're capturing the right information. Some steps for taxa may be unnecessary, others may need to be changed in order to get what we want, which is a cleaned otu table, transposed such that the otu's appear as the columns.

# restrict to only the samples in our mapfile
otu <- otu[,colnames(otu) %in% samples]   # fewer samples in otu table - why???

# check column sums for total sample reads
check <- as.data.frame(sort(colSums(otu)))
#check #view check output looks like only two samples have fewer than 500

# drop samples with low read counts
otu <- otu[,colSums(otu) > 2500] #5 out of 160 fall below 2500

# drop otus with zero counts
otu <- otu[rowSums(otu) >= 1,]

# normalize for relative abundance of otu per sample (colSums should be 100% - 1.0)
otu_norm <- sweep(otu, 2, colSums(otu), "/") #sweep by columns(2), dividing to get relative abund.("/")
# check that colsums are 1 after this:
#otuNormCheck <-as.data.frame(colSums(otu_norm))
#nrow(otuNormCheck) == colSums(otuNormCheck)

# drop low abundance
otu_norm_dla <- otu_norm[rowMeans(otu_norm) >= 0.0001,]  #removes otu with an avg. abundance less than .01%
otu_a <- otu[rownames(otu) %in% rownames(otu_norm_dla),]   #reduce otu file ???

# normalize again after removal
otu_norm_a <- sweep(otu_a, 2, colSums(otu_a), "/") 


# create clr version - make sure it is converted to integer
otu_clr_a <- otu_a*1.0

# transpose for imputation
otu_clr_a <- t(otu_clr_a)

# CLR Transformation (imputation)

```

## Diversity Analyses

```{r alpha div}
# ALPHA DIVERSITY

#######################################
# Questions here:
## 1. Do we want to do the alpha diversity based on data from a single time point? by group (time of intervention or normal/overweight)? per sample (including every time point) (doesn't seem to make sense)?
## 2. How should we group samples in the diversity tests/plots?
## 3. Do we need to include more tests beyond Shannon and Simpson? Which is best for this data?
#######################################

# Make counts table (we typically use only the otu tables here)
## multiply re-normalized reduced table by a factor to remove decimals and round

# Shannon Diversity
## We typically use shannon diversity measurements, accounting for the richness and evenness of the samples.
## diversity(index = "shannon", MARGIN = ) from vegan package (margin: per sample)

# Simpson Diversity
## We can also try Simpson/InvSimpson measurement, compare to Shannon to see which is more suited to the data.
## diversity(index = "invsimpson", MARGIN = ) from vegan package (margin: per sample)

# Inverse Simpson Diversity
## diversity(index = "simpson", MARGIN = ) from vegan package (margin: per sample)

# Plotting

```

```{r beta div}
# BETA DIVERSITY / PCoA

# Bray-Curtis
## get a distance matrix, method 1
bray <- vegdist() #expects transposed counts table

# Weighted UniFrac
## get a distance matrix, method 2
## create phyloseq object combining otu abundance and phylogenetic tree
phyobj <- phyloseq() #expects counts table and tree
UniFrac(phyobj, weighted = TRUE, normalized = TRUE)

# Unweighted UniFrac
## get a distance matrix, method 3
UniFrac(phyobj, weighted = FALSE)

# Principal Coordinate Analysis Plots
```